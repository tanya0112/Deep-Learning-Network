{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanya0112/Deep-Learning-Network/blob/main/Tanya_Saxena_DLL_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-------------------------------------------School of Electronics and Communication Engineering-------------------------------------------**\n",
        "\n",
        "**-----------------------------------------------------------PG Diploma in AI ML (ECE)-----------------------------------------------------**\n",
        "\n",
        "**---------------------------------------------------------------Deep Learning Lab-----------------------------------------------------**\n",
        "\n",
        "**Course Code: ECE6030B**\n",
        "\n",
        "**Python Codes for DLN Problems**\n",
        "\n",
        "**PGD-AI & ML(2023-24)**\n",
        "\n",
        "**Name: TANYA SAXENA**\n",
        "\n",
        "**PRN: 1032232001**"
      ],
      "metadata": {
        "id": "mEaXuZUKAhvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Implementation of different gates using the MP-model.**"
      ],
      "metadata": {
        "id": "k8N93Rlx_-_u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpYzWx6o_3WC",
        "outputId": "8e1cbac6-2308-4a69-be24-84abcf7a30d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate:\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "\n",
            "OR Gate:\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "\n",
            "NOT Gate:\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "class MPNeuron:\n",
        "    def __init__(self, weights, threshold):\n",
        "        self.weights = weights\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def activate(self, inputs):\n",
        "        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs))\n",
        "        if weighted_sum >= self.threshold:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# Implement the AND gate\n",
        "def AND_gate(inputs):\n",
        "    weights = [1, 1]  # Weights for the two inputs\n",
        "    threshold = 2    # Threshold for the AND gate\n",
        "    and_neuron = MPNeuron(weights, threshold)\n",
        "    return and_neuron.activate(inputs)\n",
        "\n",
        "# Implement the OR gate\n",
        "def OR_gate(inputs):\n",
        "    weights = [1, 1]  # Weights for the two inputs\n",
        "    threshold = 1    # Threshold for the OR gate\n",
        "    or_neuron = MPNeuron(weights, threshold)\n",
        "    return or_neuron.activate(inputs)\n",
        "\n",
        "# Implement the NOT gate\n",
        "def NOT_gate(input):\n",
        "    weight = -1      # Weight for the input\n",
        "    threshold = 0    # Threshold for the NOT gate\n",
        "    not_neuron = MPNeuron([weight], threshold)\n",
        "    return not_neuron.activate([input])\n",
        "\n",
        "# Test the gates\n",
        "print(\"AND Gate:\")\n",
        "print(AND_gate([0, 0]))  # Should output 0\n",
        "print(AND_gate([0, 1]))  # Should output 0\n",
        "print(AND_gate([1, 0]))  # Should output 0\n",
        "print(AND_gate([1, 1]))  # Should output 1\n",
        "\n",
        "print(\"\\nOR Gate:\")\n",
        "print(OR_gate([0, 0]))   # Should output 0\n",
        "print(OR_gate([0, 1]))   # Should output 1\n",
        "print(OR_gate([1, 0]))   # Should output 1\n",
        "print(OR_gate([1, 1]))   # Should output 1\n",
        "\n",
        "print(\"\\nNOT Gate:\")\n",
        "print(NOT_gate(0))  # Should output 1\n",
        "print(NOT_gate(1))  # Should output 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Implementation of the AND gate using Perceptron.**"
      ],
      "metadata": {
        "id": "1HO9e-jqZFmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, num_inputs, learning_rate=0.1, epochs=100):\n",
        "        self.weights = [0.0] * (num_inputs + 1)  # Initialize weights, including bias\n",
        "        self.learning_rate = learning_rate  # Learning rate for weight updates\n",
        "        self.epochs = epochs  # Number of training iterations\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        # Calculate the weighted sum of inputs and bias\n",
        "        weighted_sum = sum(w * x for w, x in zip(self.weights[:-1], inputs)) + self.weights[-1]\n",
        "        # Apply step function (activation function)\n",
        "        return 1 if weighted_sum >= 0 else 0\n",
        "\n",
        "    def train(self, training_data):\n",
        "        for _ in range(self.epochs):\n",
        "            for inputs, target in training_data:\n",
        "                prediction = self.predict(inputs)\n",
        "                error = target - prediction\n",
        "                # Update weights and bias\n",
        "                self.weights[:-1] = [w + self.learning_rate * error * x for w, x in zip(self.weights[:-1], inputs)]\n",
        "                self.weights[-1] += self.learning_rate * error\n",
        "\n",
        "# Define the AND gate training data\n",
        "and_gate_data = [\n",
        "    ([0, 0], 0),\n",
        "    ([0, 1], 0),\n",
        "    ([1, 0], 0),\n",
        "    ([1, 1], 1),\n",
        "]\n",
        "\n",
        "# Create a Perceptron for the AND gate\n",
        "and_perceptron = Perceptron(num_inputs=2)\n",
        "\n",
        "# Train the Perceptron on the AND gate data\n",
        "and_perceptron.train(and_gate_data)\n",
        "\n",
        "# Test the trained AND gate\n",
        "print(\"Testing the AND gate:\")\n",
        "print(and_perceptron.predict([0, 0]))  # Should output 0\n",
        "print(and_perceptron.predict([0, 1]))  # Should output 0\n",
        "print(and_perceptron.predict([1, 0]))  # Should output 0\n",
        "print(and_perceptron.predict([1, 1]))  # Should output 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxMO6s0haW4G",
        "outputId": "32b64f9e-36dc-4496-892b-628167c14748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the AND gate:\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Linear Regression using Perceptron.**"
      ],
      "metadata": {
        "id": "Mc8zemY8ajid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent for num_iterations\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Linear model\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Sample data for linear regression\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([3, 5, 7, 9])\n",
        "\n",
        "# Create a LinearRegression model\n",
        "lr_model = LinearRegression(learning_rate=0.01, num_iterations=1000)\n",
        "\n",
        "# Fit the model to the data\n",
        "lr_model.fit(X, y)\n",
        "\n",
        "# Predict using the trained model\n",
        "X_test = np.array([[5, 6], [6, 7]])\n",
        "predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkRnqkMzaj73",
        "outputId": "9fa861f5-c747-4b3f-8859-bcfeb6161632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [10.9535856  12.93160874]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Gradient Descent algorithm using Perceptron."
      ],
      "metadata": {
        "id": "KVqZp4NmnnBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the gradient descent algorithm\n",
        "def gradient_descent(W, b, alpha, dW, db):\n",
        "    '''\n",
        "    Gradient Descent Algorithm:\n",
        "    W : Weight term\n",
        "    b : bias term\n",
        "    alpha : learning rate\n",
        "    dW : partial derivative of W value\n",
        "    db : partial derivative of b value\n",
        "    '''\n",
        "    # Update the weights and bias using the learning rate and partial derivatives\n",
        "    W = W - alpha * dW\n",
        "    b = b - alpha * db\n",
        "    return W, b\n",
        "\n",
        "# Define the cost (mean squared error) function\n",
        "def cost(W, b, y, m):\n",
        "    return 1/m * np.sum((np.dot(W.T, X.T) + b - y.T)**2)\n",
        "\n",
        "# Create input data X and target data y\n",
        "X = np.array([i for i in range(5)]).reshape(-1, 1)\n",
        "\n",
        "arr1 = np.random.random(5)\n",
        "y = (2 * np.array([i for i in range(5)]) + arr1 * 1).reshape(-1, 1)\n",
        "\n",
        "print(\"Input data (X):\\n\", X)\n",
        "print(\"Target data (y):\\n\", y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.array([0]).reshape(-1, 1)\n",
        "b = np.array([0]).reshape(-1, 1)\n",
        "\n",
        "# Define the number of iterations and learning rate\n",
        "iterations = 7\n",
        "alpha = 0.01\n",
        "\n",
        "COST = []\n",
        "\n",
        "# Perform gradient descent iterations\n",
        "for i in range(iterations):\n",
        "    # Calculate partial derivatives with respect to W and b\n",
        "    dW = 1 * np.dot(np.dot(W.T, X.T) + b - y.T, X)\n",
        "    db = 1 * np.sum(np.dot(W.T, X.T) + b - y.T)\n",
        "\n",
        "    # Update weights and bias using the gradient descent algorithm\n",
        "    W, b = gradient_descent(W, b, alpha, dW, db)\n",
        "\n",
        "    # Calculate the cost and store it in the COST list\n",
        "    COST.append(cost(W, b, y, 5))\n",
        "\n",
        "    # Print the updated weights, bias, and cost at each iteration\n",
        "    print(\"Iteration:\", i + 1)\n",
        "    print(\"Updated weights (W):\\n\", W)\n",
        "    print(\"Updated bias (b):\\n\", b)\n",
        "    print(\"Cost:\", COST)\n",
        "\n",
        "# Print the final trained weights, bias, and cost\n",
        "print(\"Final trained weights (W):\\n\", W)\n",
        "print(\"Final trained bias (b):\\n\", b)\n",
        "print(\"Final cost:\", COST)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmiddfc_nk_k",
        "outputId": "d9466433-6688-487c-ccaf-d99cf99703d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data (X):\n",
            " [[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]]\n",
            "Target data (y):\n",
            " [[0.083965  ]\n",
            " [2.71184313]\n",
            " [4.42074368]\n",
            " [6.05473345]\n",
            " [8.81243269]]\n",
            "Iteration: 1\n",
            "Updated weights (W):\n",
            " [[0.64967262]]\n",
            "Updated bias (b):\n",
            " [[0.22083718]]\n",
            "Cost: [12.56593786726458]\n",
            "Iteration: 2\n",
            "Updated weights (W):\n",
            " [[1.08235973]]\n",
            "Updated bias (b):\n",
            " [[0.36566524]]\n",
            "Cost: [12.56593786726458, 5.633150619122569]\n",
            "Iteration: 3\n",
            "Updated weights (W):\n",
            " [[1.3707579]]\n",
            "Updated bias (b):\n",
            " [[0.45998318]]\n",
            "Cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704]\n",
            "Iteration: 4\n",
            "Updated weights (W):\n",
            " [[1.56320483]]\n",
            "Updated bias (b):\n",
            " [[0.52074541]]\n",
            "Cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704, 1.2106830358088068]\n",
            "Iteration: 5\n",
            "Updated weights (W):\n",
            " [[1.69184145]]\n",
            "Updated bias (b):\n",
            " [[0.55922484]]\n",
            "Cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704, 1.2106830358088068, 0.6101255365827182]\n",
            "Iteration: 6\n",
            "Updated weights (W):\n",
            " [[1.77803915]]\n",
            "Updated bias (b):\n",
            " [[0.58291663]]\n",
            "Cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704, 1.2106830358088068, 0.6101255365827182, 0.3437820568162443]\n",
            "Iteration: 7\n",
            "Updated weights (W):\n",
            " [[1.83600836]]\n",
            "Updated bias (b):\n",
            " [[0.59680406]]\n",
            "Cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704, 1.2106830358088068, 0.6101255365827182, 0.3437820568162443, 0.22522607275157266]\n",
            "Final trained weights (W):\n",
            " [[1.83600836]]\n",
            "Final trained bias (b):\n",
            " [[0.59680406]]\n",
            "Final cost: [12.56593786726458, 5.633150619122569, 2.5671167818384704, 1.2106830358088068, 0.6101255365827182, 0.3437820568162443, 0.22522607275157266]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) XoR gate implementation employing backpropagation."
      ],
      "metadata": {
        "id": "mccg7HuknkhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define the XOR gate training data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize weights and biases for the neural network\n",
        "input_size = 2\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "# Randomly initialize weights with values between -1 and 1\n",
        "input_layer_weights = 2 * np.random.random((input_size, hidden_size)) - 1\n",
        "hidden_layer_weights = 2 * np.random.random((hidden_size, output_size)) - 1\n",
        "\n",
        "# Initialize biases\n",
        "input_layer_bias = np.zeros((1, hidden_size))\n",
        "hidden_layer_bias = np.zeros((1, output_size))\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Training the neural network\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    input_layer_output = sigmoid(np.dot(X, input_layer_weights) + input_layer_bias)\n",
        "    output_layer_output = sigmoid(np.dot(input_layer_output, hidden_layer_weights) + hidden_layer_bias)\n",
        "\n",
        "    # Calculate the error (Mean Squared Error)\n",
        "    error = y - output_layer_output\n",
        "\n",
        "    # Backpropagation\n",
        "    delta_output = error * sigmoid_derivative(output_layer_output)\n",
        "    error_hidden_layer = delta_output.dot(hidden_layer_weights.T)\n",
        "    delta_hidden_layer = error_hidden_layer * sigmoid_derivative(input_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    hidden_layer_weights += input_layer_output.T.dot(delta_output) * learning_rate\n",
        "    input_layer_weights += X.T.dot(delta_hidden_layer) * learning_rate\n",
        "    hidden_layer_bias += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
        "    input_layer_bias += np.sum(delta_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Testing the trained model\n",
        "input_layer_output = sigmoid(np.dot(X, input_layer_weights) + input_layer_bias)\n",
        "output_layer_output = sigmoid(np.dot(input_layer_output, hidden_layer_weights) + hidden_layer_bias)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions after training:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} | Output: {output_layer_output[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1PCQoNCn77L",
        "outputId": "e595a63e-b882-4d46-9f36-083e262706e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions after training:\n",
            "Input: [0 0] | Output: [0.0277709]\n",
            "Input: [0 1] | Output: [0.95646053]\n",
            "Input: [1 0] | Output: [0.95832109]\n",
            "Input: [1 1] | Output: [0.05235265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Classification of MNIST digit dataset using Backpropagation (make sure to utilize the Keras library)."
      ],
      "metadata": {
        "id": "tFZcsXPRoCze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Step 2: Load and preprocess the dataset\n",
        "# Load the MNIST dataset and preprocess it\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten the input images and normalize pixel values to the range [0, 1]\n",
        "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Step 3: Build the neural network model\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "# Input layer with 128 units and ReLU activation\n",
        "model.add(Dense(128, input_shape=(28 * 28,), activation='relu'))\n",
        "\n",
        "# Hidden layer with 64 units and ReLU activation\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer with 10 units (for 10 classes) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Step 4: Compile the model\n",
        "# Compile the model with an Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "# Evaluate the model's performance on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1OxWttUoDdD",
        "outputId": "d5148dfe-abef-4582-9f8c-da3eaff0357a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 10s 4ms/step - loss: 0.3017 - accuracy: 0.9144 - val_loss: 0.1546 - val_accuracy: 0.9554\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.1258 - accuracy: 0.9631 - val_loss: 0.1193 - val_accuracy: 0.9647\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0876 - accuracy: 0.9735 - val_loss: 0.1054 - val_accuracy: 0.9687\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0669 - accuracy: 0.9789 - val_loss: 0.1014 - val_accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0515 - accuracy: 0.9836 - val_loss: 0.0976 - val_accuracy: 0.9706\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0413 - accuracy: 0.9875 - val_loss: 0.0895 - val_accuracy: 0.9733\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.0335 - accuracy: 0.9899 - val_loss: 0.0931 - val_accuracy: 0.9734\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0288 - accuracy: 0.9906 - val_loss: 0.1140 - val_accuracy: 0.9695\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0984 - val_accuracy: 0.9748\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1083 - val_accuracy: 0.9718\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0927 - accuracy: 0.9744\n",
            "Test Loss: 0.0927, Test Accuracy: 0.9744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Exploring and implementing variants of the Gradient Descent algorithm in Python.\n",
        "\n",
        "A)Mini-batch Gradient Descent"
      ],
      "metadata": {
        "id": "B-nOWbX-oQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent_mini_batch(W, b, alpha, dW, db):\n",
        "    '''\n",
        "    Mini-batch Gradient Descent Algorithm:\n",
        "    W : Weight term\n",
        "    b : bias term\n",
        "    alpha : learning rate\n",
        "    dW : partial derivative of W value\n",
        "    db : partial derivative of b value\n",
        "    '''\n",
        "    # Update weights and bias using the learning rate and partial derivatives\n",
        "    W = W - alpha * dW\n",
        "    b = b - alpha * db\n",
        "    return W, b\n",
        "\n",
        "def cost(W, b, X, y, m):\n",
        "    return 1 / (2 * m) * np.sum((np.dot(X, W) + b - y) ** 2)\n",
        "\n",
        "# Define input data X and target data y\n",
        "X = np.array([i for i in range(100)]).reshape(-1, 1)\n",
        "y = 2 * X + np.random.randn(100, 1)  # Adding some noise for a more realistic scenario\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.array([0]).reshape(-1, 1)\n",
        "b = np.array([0]).reshape(-1, 1)\n",
        "\n",
        "batch_size = 10\n",
        "iterations = 100\n",
        "alpha = 0.01\n",
        "\n",
        "COST = []\n",
        "\n",
        "# Perform Mini-batch Gradient Descent iterations\n",
        "m = len(X)\n",
        "for i in range(iterations):\n",
        "    # Randomly select a mini-batch from the training data\n",
        "    indices = np.random.choice(m, batch_size, replace=False)\n",
        "    X_mini_batch = X[indices]\n",
        "    y_mini_batch = y[indices]\n",
        "\n",
        "    # Calculate partial derivatives for the mini-batch\n",
        "    predictions = np.dot(X_mini_batch, W) + b\n",
        "    dW = (1 / batch_size) * np.dot(X_mini_batch.T, predictions - y_mini_batch)\n",
        "    db = (1 / batch_size) * np.sum(predictions - y_mini_batch)\n",
        "\n",
        "    # Update weights and bias using the Mini-batch Gradient Descent algorithm\n",
        "    W, b = gradient_descent_mini_batch(W, b, alpha, dW.reshape(-1, 1), db)\n",
        "\n",
        "    # Calculate the cost and store it in the COST list\n",
        "    J = cost(W, b, X_mini_batch, y_mini_batch, batch_size)\n",
        "    COST.append(J)\n",
        "\n",
        "    print(\"Iteration:\", i + 1)\n",
        "    print(\"Updated weights (W):\\n\", W)\n",
        "    print(\"Updated bias (b):\\n\", b)\n",
        "    print(\"Cost:\", J)\n",
        "\n",
        "# Print the final trained weights, bias, and cost\n",
        "print(\"Final trained weights (W):\\n\", W)\n",
        "print(\"Final trained bias (b):\\n\", b)\n",
        "print(\"Final cost:\", COST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG9Oa0TAoTI6",
        "outputId": "31f8923d-ba17-49e1-f500-5fae35bcf54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Updated weights (W):\n",
            " [[61.56148789]]\n",
            "Updated bias (b):\n",
            " [[1.00231823]]\n",
            "Cost: 5457388.1585912565\n",
            "Iteration: 2\n",
            "Updated weights (W):\n",
            " [[-2196.74040122]]\n",
            "Updated bias (b):\n",
            " [[-32.41656834]]\n",
            "Cost: 9167513969.508986\n",
            "Iteration: 3\n",
            "Updated weights (W):\n",
            " [[57775.04936004]]\n",
            "Updated bias (b):\n",
            " [[933.15812747]]\n",
            "Cost: 4553187995652.865\n",
            "Iteration: 4\n",
            "Updated weights (W):\n",
            " [[-2826884.1742514]]\n",
            "Updated bias (b):\n",
            " [[-38015.20400843]]\n",
            "Cost: 1.995349240264682e+16\n",
            "Iteration: 5\n",
            "Updated weights (W):\n",
            " [[98838507.56921323]]\n",
            "Updated bias (b):\n",
            " [[1378634.91876483]]\n",
            "Cost: 1.7570114559487934e+19\n",
            "Iteration: 6\n",
            "Updated weights (W):\n",
            " [[-3.0808539e+09]]\n",
            "Updated bias (b):\n",
            " [[-45286926.06692203]]\n",
            "Cost: 1.5271075340843324e+22\n",
            "Iteration: 7\n",
            "Updated weights (W):\n",
            " [[8.16840754e+10]]\n",
            "Updated bias (b):\n",
            " [[1.30766081e+09]]\n",
            "Cost: 9.181421572089821e+24\n",
            "Iteration: 8\n",
            "Updated weights (W):\n",
            " [[-3.23982738e+12]]\n",
            "Updated bias (b):\n",
            " [[-4.41217617e+10]]\n",
            "Cost: 2.1344146728629116e+28\n",
            "Iteration: 9\n",
            "Updated weights (W):\n",
            " [[1.11497317e+14]]\n",
            "Updated bias (b):\n",
            " [[1.76090331e+12]]\n",
            "Cost: 2.201931190789319e+31\n",
            "Iteration: 10\n",
            "Updated weights (W):\n",
            " [[-2.55849859e+15]]\n",
            "Updated bias (b):\n",
            " [[-4.57545626e+13]]\n",
            "Cost: 7.840448218096575e+33\n",
            "Iteration: 11\n",
            "Updated weights (W):\n",
            " [[4.41413869e+16]]\n",
            "Updated bias (b):\n",
            " [[7.91332021e+14]]\n",
            "Cost: 1.778821880032892e+36\n",
            "Iteration: 12\n",
            "Updated weights (W):\n",
            " [[-1.31530999e+18]]\n",
            "Updated bias (b):\n",
            " [[-2.05368712e+16]]\n",
            "Cost: 2.6646155454679722e+39\n",
            "Iteration: 13\n",
            "Updated weights (W):\n",
            " [[3.46050178e+19]]\n",
            "Updated bias (b):\n",
            " [[6.03125432e+17]]\n",
            "Cost: 1.6357066291165344e+42\n",
            "Iteration: 14\n",
            "Updated weights (W):\n",
            " [[-5.00773279e+20]]\n",
            "Updated bias (b):\n",
            " [[-1.15492671e+19]]\n",
            "Cost: 1.9411397025515903e+44\n",
            "Iteration: 15\n",
            "Updated weights (W):\n",
            " [[1.39734395e+22]]\n",
            "Updated bias (b):\n",
            " [[2.214258e+20]]\n",
            "Cost: 2.8222194018490012e+47\n",
            "Iteration: 16\n",
            "Updated weights (W):\n",
            " [[-5.08989651e+23]]\n",
            "Updated bias (b):\n",
            " [[-7.28452546e+21]]\n",
            "Cost: 4.848811123577582e+50\n",
            "Iteration: 17\n",
            "Updated weights (W):\n",
            " [[1.19209429e+25]]\n",
            "Updated bias (b):\n",
            " [[1.98929129e+23]]\n",
            "Cost: 1.7357547775859882e+53\n",
            "Iteration: 18\n",
            "Updated weights (W):\n",
            " [[-3.1391867e+26]]\n",
            "Updated bias (b):\n",
            " [[-5.07211693e+24]]\n",
            "Cost: 1.3471232922499858e+56\n",
            "Iteration: 19\n",
            "Updated weights (W):\n",
            " [[1.17047207e+28]]\n",
            "Updated bias (b):\n",
            " [[1.66692117e+26]]\n",
            "Cost: 2.6230511998130164e+59\n",
            "Iteration: 20\n",
            "Updated weights (W):\n",
            " [[-2.7504118e+29]]\n",
            "Updated bias (b):\n",
            " [[-4.20083563e+27]]\n",
            "Cost: 9.268500060343935e+61\n",
            "Iteration: 21\n",
            "Updated weights (W):\n",
            " [[6.86109615e+30]]\n",
            "Updated bias (b):\n",
            " [[1.16584251e+29]]\n",
            "Cost: 6.108857703515734e+64\n",
            "Iteration: 22\n",
            "Updated weights (W):\n",
            " [[-1.89366375e+32]]\n",
            "Updated bias (b):\n",
            " [[-2.7182143e+30]]\n",
            "Cost: 5.128797398936719e+67\n",
            "Iteration: 23\n",
            "Updated weights (W):\n",
            " [[6.42216765e+33]]\n",
            "Updated bias (b):\n",
            " [[1.02596673e+32]]\n",
            "Cost: 7.202020709368235e+70\n",
            "Iteration: 24\n",
            "Updated weights (W):\n",
            " [[-2.7969907e+35]]\n",
            "Updated bias (b):\n",
            " [[-3.97008358e+33]]\n",
            "Cost: 1.7429993814260416e+74\n",
            "Iteration: 25\n",
            "Updated weights (W):\n",
            " [[6.02145187e+36]]\n",
            "Updated bias (b):\n",
            " [[1.00956769e+35]]\n",
            "Cost: 4.0854616047501396e+76\n",
            "Iteration: 26\n",
            "Updated weights (W):\n",
            " [[-1.86071696e+38]]\n",
            "Updated bias (b):\n",
            " [[-2.94088599e+36]]\n",
            "Cost: 5.5238688886067256e+79\n",
            "Iteration: 27\n",
            "Updated weights (W):\n",
            " [[4.17194106e+39]]\n",
            "Updated bias (b):\n",
            " [[6.94704125e+37]]\n",
            "Cost: 2.0388288395741395e+82\n",
            "Iteration: 28\n",
            "Updated weights (W):\n",
            " [[-1.59099023e+41]]\n",
            "Updated bias (b):\n",
            " [[-2.3092307e+39]]\n",
            "Cost: 4.95397863526926e+85\n",
            "Iteration: 29\n",
            "Updated weights (W):\n",
            " [[5.35435252e+42]]\n",
            "Updated bias (b):\n",
            " [[7.67860761e+40]]\n",
            "Cost: 4.968533619319182e+88\n",
            "Iteration: 30\n",
            "Updated weights (W):\n",
            " [[-2.37299275e+44]]\n",
            "Updated bias (b):\n",
            " [[-3.26509776e+42]]\n",
            "Cost: 1.2762080071492998e+92\n",
            "Iteration: 31\n",
            "Updated weights (W):\n",
            " [[9.17614387e+45]]\n",
            "Updated bias (b):\n",
            " [[1.36062227e+44]]\n",
            "Cost: 1.670493129190668e+95\n",
            "Iteration: 32\n",
            "Updated weights (W):\n",
            " [[-3.30388349e+47]]\n",
            "Updated bias (b):\n",
            " [[-4.90300138e+45]]\n",
            "Cost: 2.0201202384288888e+98\n",
            "Iteration: 33\n",
            "Updated weights (W):\n",
            " [[1.18544973e+49]]\n",
            "Updated bias (b):\n",
            " [[1.61992145e+47]]\n",
            "Cost: 2.5918347492662118e+101\n",
            "Iteration: 34\n",
            "Updated weights (W):\n",
            " [[-4.417841e+50]]\n",
            "Updated bias (b):\n",
            " [[-6.09880233e+48]]\n",
            "Cost: 3.735085943460674e+104\n",
            "Iteration: 35\n",
            "Updated weights (W):\n",
            " [[1.64732451e+52]]\n",
            "Updated bias (b):\n",
            " [[2.17946724e+50]]\n",
            "Cost: 5.195935977397697e+107\n",
            "Iteration: 36\n",
            "Updated weights (W):\n",
            " [[-4.09229771e+53]]\n",
            "Updated bias (b):\n",
            " [[-7.36192549e+51]]\n",
            "Cost: 2.1647507356942994e+110\n",
            "Iteration: 37\n",
            "Updated weights (W):\n",
            " [[6.86000314e+54]]\n",
            "Updated bias (b):\n",
            " [[1.33895965e+53]]\n",
            "Cost: 4.181360223233267e+112\n",
            "Iteration: 38\n",
            "Updated weights (W):\n",
            " [[-2.68956152e+56]]\n",
            "Updated bias (b):\n",
            " [[-3.88054483e+54]]\n",
            "Cost: 1.4544117711227586e+116\n",
            "Iteration: 39\n",
            "Updated weights (W):\n",
            " [[5.84158073e+57]]\n",
            "Updated bias (b):\n",
            " [[9.78236861e+55]]\n",
            "Cost: 3.877630656228544e+118\n",
            "Iteration: 40\n",
            "Updated weights (W):\n",
            " [[-1.88586615e+59]]\n",
            "Updated bias (b):\n",
            " [[-2.86483598e+57]]\n",
            "Cost: 5.919851692517775e+121\n",
            "Iteration: 41\n",
            "Updated weights (W):\n",
            " [[4.87449573e+60]]\n",
            "Updated bias (b):\n",
            " [[7.80674701e+58]]\n",
            "Cost: 3.1904387170456205e+124\n",
            "Iteration: 42\n",
            "Updated weights (W):\n",
            " [[-1.91112059e+62]]\n",
            "Updated bias (b):\n",
            " [[-2.82303816e+60]]\n",
            "Cost: 7.343950317573269e+127\n",
            "Iteration: 43\n",
            "Updated weights (W):\n",
            " [[7.59769216e+63]]\n",
            "Updated bias (b):\n",
            " [[1.02890161e+62]]\n",
            "Cost: 1.1764911304036057e+131\n",
            "Iteration: 44\n",
            "Updated weights (W):\n",
            " [[-2.93821254e+65]]\n",
            "Updated bias (b):\n",
            " [[-4.06927173e+63]]\n",
            "Cost: 1.7128146429304795e+134\n",
            "Iteration: 45\n",
            "Updated weights (W):\n",
            " [[1.32247122e+67]]\n",
            "Updated bias (b):\n",
            " [[1.7373328e+65]]\n",
            "Cost: 4.0240168968178625e+137\n",
            "Iteration: 46\n",
            "Updated weights (W):\n",
            " [[-3.31105429e+68]]\n",
            "Updated bias (b):\n",
            " [[-5.73945039e+66]]\n",
            "Cost: 1.4277493692088666e+140\n",
            "Iteration: 47\n",
            "Updated weights (W):\n",
            " [[1.2072742e+70]]\n",
            "Updated bias (b):\n",
            " [[1.6483724e+68]]\n",
            "Cost: 2.7304329737884817e+143\n",
            "Iteration: 48\n",
            "Updated weights (W):\n",
            " [[-3.49215858e+71]]\n",
            "Updated bias (b):\n",
            " [[-6.0059823e+69]]\n",
            "Cost: 1.8254086324148988e+146\n",
            "Iteration: 49\n",
            "Updated weights (W):\n",
            " [[1.42057612e+73]]\n",
            "Updated bias (b):\n",
            " [[1.97646923e+71]]\n",
            "Cost: 4.206115283572144e+149\n",
            "Iteration: 50\n",
            "Updated weights (W):\n",
            " [[-5.77456882e+74]]\n",
            "Updated bias (b):\n",
            " [[-7.44702908e+72]]\n",
            "Cost: 6.94521060620311e+152\n",
            "Iteration: 51\n",
            "Updated weights (W):\n",
            " [[1.88369469e+76]]\n",
            "Updated bias (b):\n",
            " [[2.84820623e+74]]\n",
            "Cost: 5.966352557425488e+155\n",
            "Iteration: 52\n",
            "Updated weights (W):\n",
            " [[-5.45017089e+77]]\n",
            "Updated bias (b):\n",
            " [[-8.94813158e+75]]\n",
            "Cost: 4.44705387023434e+158\n",
            "Iteration: 53\n",
            "Updated weights (W):\n",
            " [[2.49887204e+79]]\n",
            "Updated bias (b):\n",
            " [[3.37227201e+77]]\n",
            "Cost: 1.4629333995976152e+162\n",
            "Iteration: 54\n",
            "Updated weights (W):\n",
            " [[-1.05268063e+81]]\n",
            "Updated bias (b):\n",
            " [[-1.49092645e+79]]\n",
            "Cost: 2.3899890085054806e+165\n",
            "Iteration: 55\n",
            "Updated weights (W):\n",
            " [[2.74227833e+82]]\n",
            "Updated bias (b):\n",
            " [[4.63156836e+80]]\n",
            "Cost: 1.0174433610249653e+168\n",
            "Iteration: 56\n",
            "Updated weights (W):\n",
            " [[-8.19818617e+83]]\n",
            "Updated bias (b):\n",
            " [[-1.33899803e+82]]\n",
            "Cost: 1.0385160431440766e+171\n",
            "Iteration: 57\n",
            "Updated weights (W):\n",
            " [[2.67837744e+85]]\n",
            "Updated bias (b):\n",
            " [[3.56482116e+83]]\n",
            "Cost: 1.2078729507076622e+174\n",
            "Iteration: 58\n",
            "Updated weights (W):\n",
            " [[-7.63315135e+86]]\n",
            "Updated bias (b):\n",
            " [[-1.20479702e+85]]\n",
            "Cost: 8.596306807599265e+176\n",
            "Iteration: 59\n",
            "Updated weights (W):\n",
            " [[2.26970569e+88]]\n",
            "Updated bias (b):\n",
            " [[3.67440131e+86]]\n",
            "Cost: 7.918750753420038e+179\n",
            "Iteration: 60\n",
            "Updated weights (W):\n",
            " [[-8.34819736e+89]]\n",
            "Updated bias (b):\n",
            " [[-1.22104038e+88]]\n",
            "Cost: 1.3167754453490233e+183\n",
            "Iteration: 61\n",
            "Updated weights (W):\n",
            " [[1.5202521e+91]]\n",
            "Updated bias (b):\n",
            " [[3.0430838e+89]]\n",
            "Cost: 2.2210514074061216e+185\n",
            "Iteration: 62\n",
            "Updated weights (W):\n",
            " [[-3.71180914e+92]]\n",
            "Updated bias (b):\n",
            " [[-6.3422364e+90]]\n",
            "Cost: 1.751257103139727e+188\n",
            "Iteration: 63\n",
            "Updated weights (W):\n",
            " [[9.55142594e+93]]\n",
            "Updated bias (b):\n",
            " [[1.57411969e+92]]\n",
            "Cost: 1.2197205372742352e+191\n",
            "Iteration: 64\n",
            "Updated weights (W):\n",
            " [[-2.11469189e+95]]\n",
            "Updated bias (b):\n",
            " [[-3.35908689e+93]]\n",
            "Cost: 5.1752865746174675e+193\n",
            "Iteration: 65\n",
            "Updated weights (W):\n",
            " [[7.4951005e+96]]\n",
            "Updated bias (b):\n",
            " [[1.16366065e+95]]\n",
            "Cost: 1.0238615921348253e+197\n",
            "Iteration: 66\n",
            "Updated weights (W):\n",
            " [[-1.83198451e+98]]\n",
            "Updated bias (b):\n",
            " [[-2.9877692e+96]]\n",
            "Cost: 4.2706400087819406e+199\n",
            "Iteration: 67\n",
            "Updated weights (W):\n",
            " [[7.03851783e+99]]\n",
            "Updated bias (b):\n",
            " [[1.00182837e+98]]\n",
            "Cost: 9.766219608816507e+202\n",
            "Iteration: 68\n",
            "Updated weights (W):\n",
            " [[-2.02831719e+101]]\n",
            "Updated bias (b):\n",
            " [[-3.0611135e+99]]\n",
            "Cost: 6.135016245781536e+205\n",
            "Iteration: 69\n",
            "Updated weights (W):\n",
            " [[8.09222524e+102]]\n",
            "Updated bias (b):\n",
            " [[1.21305341e+101]]\n",
            "Cost: 1.33932620357916e+209\n",
            "Iteration: 70\n",
            "Updated weights (W):\n",
            " [[-3.07698569e+104]]\n",
            "Updated bias (b):\n",
            " [[-4.18497154e+102]]\n",
            "Cost: 1.847671563152089e+212\n",
            "Iteration: 71\n",
            "Updated weights (W):\n",
            " [[1.14602555e+106]]\n",
            "Updated bias (b):\n",
            " [[1.55860134e+104]]\n",
            "Cost: 2.5119697720817923e+215\n",
            "Iteration: 72\n",
            "Updated weights (W):\n",
            " [[-2.8624184e+107]]\n",
            "Updated bias (b):\n",
            " [[-4.83090961e+105]]\n",
            "Cost: 1.064560668602315e+218\n",
            "Iteration: 73\n",
            "Updated weights (W):\n",
            " [[9.56869503e+108]]\n",
            "Updated bias (b):\n",
            " [[1.42918189e+107]]\n",
            "Cost: 1.5764520536157284e+221\n",
            "Iteration: 74\n",
            "Updated weights (W):\n",
            " [[-3.22852391e+110]]\n",
            "Updated bias (b):\n",
            " [[-4.6524272e+108]]\n",
            "Cost: 1.8109266045158493e+224\n",
            "Iteration: 75\n",
            "Updated weights (W):\n",
            " [[7.29055795e+111]]\n",
            "Updated bias (b):\n",
            " [[1.23243644e+110]]\n",
            "Cost: 6.269144362259593e+226\n",
            "Iteration: 76\n",
            "Updated weights (W):\n",
            " [[-1.61630303e+113]]\n",
            "Updated bias (b):\n",
            " [[-2.81608365e+111]]\n",
            "Cost: 3.0274268651731837e+229\n",
            "Iteration: 77\n",
            "Updated weights (W):\n",
            " [[3.36418897e+114]]\n",
            "Updated bias (b):\n",
            " [[5.37826832e+112]]\n",
            "Cost: 1.2347227803144057e+232\n",
            "Iteration: 78\n",
            "Updated weights (W):\n",
            " [[-9.05551729e+115]]\n",
            "Updated bias (b):\n",
            " [[-1.50101045e+114]]\n",
            "Cost: 1.1449714820013679e+235\n",
            "Iteration: 79\n",
            "Updated weights (W):\n",
            " [[1.28419045e+117]]\n",
            "Updated bias (b):\n",
            " [[2.7491655e+115]]\n",
            "Cost: 1.2525014414463278e+237\n",
            "Iteration: 80\n",
            "Updated weights (W):\n",
            " [[-4.10463171e+118]]\n",
            "Updated bias (b):\n",
            " [[-6.80372202e+116]]\n",
            "Cost: 2.777331479746357e+240\n",
            "Iteration: 81\n",
            "Updated weights (W):\n",
            " [[1.5101099e+120]]\n",
            "Updated bias (b):\n",
            " [[2.0095868e+118]]\n",
            "Cost: 4.309499702372458e+243\n",
            "Iteration: 82\n",
            "Updated weights (W):\n",
            " [[-3.51858246e+121]]\n",
            "Updated bias (b):\n",
            " [[-6.80796086e+119]]\n",
            "Cost: 1.5049616377684066e+246\n",
            "Iteration: 83\n",
            "Updated weights (W):\n",
            " [[1.00629495e+123]]\n",
            "Updated bias (b):\n",
            " [[1.64966943e+121]]\n",
            "Cost: 1.4989956367935033e+249\n",
            "Iteration: 84\n",
            "Updated weights (W):\n",
            " [[-3.36239767e+124]]\n",
            "Updated bias (b):\n",
            " [[-5.13985713e+122]]\n",
            "Cost: 1.945777058136296e+252\n",
            "Iteration: 85\n",
            "Updated weights (W):\n",
            " [[8.22365493e+125]]\n",
            "Updated bias (b):\n",
            " [[1.42520799e+124]]\n",
            "Cost: 8.611211107609523e+254\n",
            "Iteration: 86\n",
            "Updated weights (W):\n",
            " [[-3.04102919e+127]]\n",
            "Updated bias (b):\n",
            " [[-4.15165228e+125]]\n",
            "Cost: 1.756365066779816e+258\n",
            "Iteration: 87\n",
            "Updated weights (W):\n",
            " [[9.3212475e+128]]\n",
            "Updated bias (b):\n",
            " [[1.4337978e+127]]\n",
            "Cost: 1.375396569540723e+261\n",
            "Iteration: 88\n",
            "Updated weights (W):\n",
            " [[-1.43222824e+130]]\n",
            "Updated bias (b):\n",
            " [[-3.00863567e+128]]\n",
            "Cost: 1.6794020950155616e+263\n",
            "Iteration: 89\n",
            "Updated weights (W):\n",
            " [[6.31889516e+131]]\n",
            "Updated bias (b):\n",
            " [[8.10932483e+129]]\n",
            "Cost: 9.008268066728515e+266\n",
            "Iteration: 90\n",
            "Updated weights (W):\n",
            " [[-2.57719914e+133]]\n",
            "Updated bias (b):\n",
            " [[-3.66050362e+131]]\n",
            "Cost: 1.3879966178544514e+270\n",
            "Iteration: 91\n",
            "Updated weights (W):\n",
            " [[7.24433899e+134]]\n",
            "Updated bias (b):\n",
            " [[1.11576903e+133]]\n",
            "Cost: 7.640306039211978e+272\n",
            "Iteration: 92\n",
            "Updated weights (W):\n",
            " [[-3.80790186e+136]]\n",
            "Updated bias (b):\n",
            " [[-4.98230918e+134]]\n",
            "Cost: 3.8839575746185994e+276\n",
            "Iteration: 93\n",
            "Updated weights (W):\n",
            " [[1.06487698e+138]]\n",
            "Updated bias (b):\n",
            " [[1.67565468e+136]]\n",
            "Cost: 1.642730155129841e+279\n",
            "Iteration: 94\n",
            "Updated weights (W):\n",
            " [[-3.68496758e+139]]\n",
            "Updated bias (b):\n",
            " [[-4.99876353e+137]]\n",
            "Cost: 2.417749338395982e+282\n",
            "Iteration: 95\n",
            "Updated weights (W):\n",
            " [[9.76743352e+140]]\n",
            "Updated bias (b):\n",
            " [[1.62348752e+139]]\n",
            "Cost: 1.312507968804846e+285\n",
            "Iteration: 96\n",
            "Updated weights (W):\n",
            " [[-2.89691287e+142]]\n",
            "Updated bias (b):\n",
            " [[-4.47880566e+140]]\n",
            "Cost: 1.2867483229722702e+288\n",
            "Iteration: 97\n",
            "Updated weights (W):\n",
            " [[9.97147971e+143]]\n",
            "Updated bias (b):\n",
            " [[1.46495143e+142]]\n",
            "Cost: 1.7613256829046963e+291\n",
            "Iteration: 98\n",
            "Updated weights (W):\n",
            " [[-4.72312975e+145]]\n",
            "Updated bias (b):\n",
            " [[-6.22674534e+143]]\n",
            "Cost: 5.395608288601033e+294\n",
            "Iteration: 99\n",
            "Updated weights (W):\n",
            " [[1.37816196e+147]]\n",
            "Updated bias (b):\n",
            " [[2.08737926e+145]]\n",
            "Cost: 2.86673392370804e+297\n",
            "Iteration: 100\n",
            "Updated weights (W):\n",
            " [[-3.48541722e+148]]\n",
            "Updated bias (b):\n",
            " [[-5.55406644e+146]]\n",
            "Cost: 1.5973169951946836e+300\n",
            "Final trained weights (W):\n",
            " [[-3.48541722e+148]]\n",
            "Final trained bias (b):\n",
            " [[-5.55406644e+146]]\n",
            "Final cost: [5457388.1585912565, 9167513969.508986, 4553187995652.865, 1.995349240264682e+16, 1.7570114559487934e+19, 1.5271075340843324e+22, 9.181421572089821e+24, 2.1344146728629116e+28, 2.201931190789319e+31, 7.840448218096575e+33, 1.778821880032892e+36, 2.6646155454679722e+39, 1.6357066291165344e+42, 1.9411397025515903e+44, 2.8222194018490012e+47, 4.848811123577582e+50, 1.7357547775859882e+53, 1.3471232922499858e+56, 2.6230511998130164e+59, 9.268500060343935e+61, 6.108857703515734e+64, 5.128797398936719e+67, 7.202020709368235e+70, 1.7429993814260416e+74, 4.0854616047501396e+76, 5.5238688886067256e+79, 2.0388288395741395e+82, 4.95397863526926e+85, 4.968533619319182e+88, 1.2762080071492998e+92, 1.670493129190668e+95, 2.0201202384288888e+98, 2.5918347492662118e+101, 3.735085943460674e+104, 5.195935977397697e+107, 2.1647507356942994e+110, 4.181360223233267e+112, 1.4544117711227586e+116, 3.877630656228544e+118, 5.919851692517775e+121, 3.1904387170456205e+124, 7.343950317573269e+127, 1.1764911304036057e+131, 1.7128146429304795e+134, 4.0240168968178625e+137, 1.4277493692088666e+140, 2.7304329737884817e+143, 1.8254086324148988e+146, 4.206115283572144e+149, 6.94521060620311e+152, 5.966352557425488e+155, 4.44705387023434e+158, 1.4629333995976152e+162, 2.3899890085054806e+165, 1.0174433610249653e+168, 1.0385160431440766e+171, 1.2078729507076622e+174, 8.596306807599265e+176, 7.918750753420038e+179, 1.3167754453490233e+183, 2.2210514074061216e+185, 1.751257103139727e+188, 1.2197205372742352e+191, 5.1752865746174675e+193, 1.0238615921348253e+197, 4.2706400087819406e+199, 9.766219608816507e+202, 6.135016245781536e+205, 1.33932620357916e+209, 1.847671563152089e+212, 2.5119697720817923e+215, 1.064560668602315e+218, 1.5764520536157284e+221, 1.8109266045158493e+224, 6.269144362259593e+226, 3.0274268651731837e+229, 1.2347227803144057e+232, 1.1449714820013679e+235, 1.2525014414463278e+237, 2.777331479746357e+240, 4.309499702372458e+243, 1.5049616377684066e+246, 1.4989956367935033e+249, 1.945777058136296e+252, 8.611211107609523e+254, 1.756365066779816e+258, 1.375396569540723e+261, 1.6794020950155616e+263, 9.008268066728515e+266, 1.3879966178544514e+270, 7.640306039211978e+272, 3.8839575746185994e+276, 1.642730155129841e+279, 2.417749338395982e+282, 1.312507968804846e+285, 1.2867483229722702e+288, 1.7613256829046963e+291, 5.395608288601033e+294, 2.86673392370804e+297, 1.5973169951946836e+300]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "EtKWxCdVo7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_parameters(dim):\n",
        "    \"\"\"\n",
        "    Initialize the weights and bias.\n",
        "    \"\"\"\n",
        "    W = np.zeros((dim, 1))\n",
        "    b = 0\n",
        "    return W, b\n",
        "\n",
        "def momentum_gradient_descent(X, y, W, b, learning_rate, beta, num_iterations, batch_size):\n",
        "    \"\"\"\n",
        "    Perform Mini-batch Gradient Descent with Momentum.\n",
        "    \"\"\"\n",
        "    m = X.shape[0]  # Number of training examples\n",
        "    v_w = np.zeros_like(W)  # Initialize velocity for weights\n",
        "    v_b = 0  # Initialize velocity for bias\n",
        "    beta = 0.9  # Momentum parameter\n",
        "\n",
        "    COST = []  # To store the cost at each iteration\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Randomly shuffle the data\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[permutation]\n",
        "        y_shuffled = y[permutation]\n",
        "\n",
        "        for j in range(0, m, batch_size):\n",
        "            # Get a mini-batch\n",
        "            X_mini_batch = X_shuffled[j:j+batch_size]\n",
        "            y_mini_batch = y_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Calculate the gradient\n",
        "            Z = np.dot(X_mini_batch, W) + b\n",
        "            dW = (1 / batch_size) * np.dot(X_mini_batch.T, Z - y_mini_batch)\n",
        "            db = (1 / batch_size) * np.sum(Z - y_mini_batch)\n",
        "\n",
        "            # Update velocity\n",
        "            v_w = beta * v_w + (1 - beta) * dW\n",
        "            v_b = beta * v_b + (1 - beta) * db\n",
        "\n",
        "            # Update weights and bias\n",
        "            W -= learning_rate * v_w\n",
        "            b -= learning_rate * v_b\n",
        "\n",
        "        # Calculate the cost and store it in COST\n",
        "        J = 0.5 * np.mean((np.dot(X, W) + b - y) ** 2)\n",
        "        COST.append(J)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Iteration {i + 1}: Cost = {J}\")\n",
        "\n",
        "    return W, b, COST\n",
        "\n",
        "# Generate some random data for demonstration\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 3 * X + 2 + 0.2 * np.random.randn(100, 1)\n",
        "\n",
        "# Initialize parameters\n",
        "W, b = initialize_parameters(X.shape[1])\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.1\n",
        "beta = 0.9\n",
        "num_iterations = 100\n",
        "batch_size = 10\n",
        "\n",
        "# Perform Mini-batch Gradient Descent with Momentum\n",
        "W, b, COST = momentum_gradient_descent(X, y, W, b, learning_rate, beta, num_iterations, batch_size)\n",
        "\n",
        "# Print the final trained weights, bias, and cost\n",
        "print(\"Final trained weights (W):\\n\", W)\n",
        "print(\"Final trained bias (b):\\n\", b)\n",
        "print(\"Final cost:\", COST[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxCF6MP3o4C_",
        "outputId": "cef68739-9d7e-44c4-e510-4972c8156b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Cost = 1.9768058711249537\n",
            "Iteration 11: Cost = 0.044653973838335796\n",
            "Iteration 21: Cost = 0.025661350901588643\n",
            "Iteration 31: Cost = 0.021227541443740686\n",
            "Iteration 41: Cost = 0.020186925805158847\n",
            "Iteration 51: Cost = 0.019922709381705848\n",
            "Iteration 61: Cost = 0.019898726936433263\n",
            "Iteration 71: Cost = 0.01985259347406723\n",
            "Iteration 81: Cost = 0.019850053669843087\n",
            "Iteration 91: Cost = 0.01989171697423979\n",
            "Final trained weights (W):\n",
            " [[2.99106558]]\n",
            "Final trained bias (b):\n",
            " 2.054547809343955\n",
            "Final cost: 0.0199196271941831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) RMSprop (Root Mean Square Propagation)"
      ],
      "metadata": {
        "id": "sOvvQhiWo3vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_parameters(dim):\n",
        "    \"\"\"\n",
        "    Initialize the weights and bias.\n",
        "    \"\"\"\n",
        "    W = np.zeros((dim, 1))\n",
        "    b = 0\n",
        "    return W, b\n",
        "\n",
        "def rmsprop_gradient_descent(X, y, W, b, learning_rate, beta, epsilon, num_iterations, batch_size):\n",
        "    \"\"\"\n",
        "    Perform Mini-batch Gradient Descent with RMSProp.\n",
        "    \"\"\"\n",
        "    m = X.shape[0]  # Number of training examples\n",
        "    cache_w = np.zeros_like(W)  # Initialize squared gradient cache for weights\n",
        "    cache_b = 0  # Initialize squared gradient cache for bias\n",
        "    beta = 0.9  # RMSProp decay parameter\n",
        "\n",
        "    COST = []  # To store the cost at each iteration\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Randomly shuffle the data\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[permutation]\n",
        "        y_shuffled = y[permutation]\n",
        "\n",
        "        for j in range(0, m, batch_size):\n",
        "            # Get a mini-batch\n",
        "            X_mini_batch = X_shuffled[j:j+batch_size]\n",
        "            y_mini_batch = y_shuffled[j:j+batch_size]\n",
        "\n",
        "            # Calculate the gradient\n",
        "            Z = np.dot(X_mini_batch, W) + b\n",
        "            dW = (1 / batch_size) * np.dot(X_mini_batch.T, Z - y_mini_batch)\n",
        "            db = (1 / batch_size) * np.sum(Z - y_mini_batch)\n",
        "\n",
        "            # Update the squared gradient cache\n",
        "            cache_w = beta * cache_w + (1 - beta) * (dW**2)\n",
        "            cache_b = beta * cache_b + (1 - beta) * (db**2)\n",
        "\n",
        "            # Update weights and bias using RMSProp\n",
        "            W -= (learning_rate / np.sqrt(cache_w + epsilon)) * dW\n",
        "            b -= (learning_rate / np.sqrt(cache_b + epsilon)) * db\n",
        "\n",
        "        # Calculate the cost and store it in COST\n",
        "        J = 0.5 * np.mean((np.dot(X, W) + b - y) ** 2)\n",
        "        COST.append(J)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Iteration {i + 1}: Cost = {J}\")\n",
        "\n",
        "    return W, b, COST\n",
        "\n",
        "# Generate some random data for demonstration\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 3 * X + 2 + 0.2 * np.random.randn(100, 1)\n",
        "\n",
        "# Initialize parameters\n",
        "W, b = initialize_parameters(X.shape[1])\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "beta = 0.9\n",
        "epsilon = 1e-8\n",
        "num_iterations = 100\n",
        "batch_size = 10\n",
        "\n",
        "# Perform Mini-batch Gradient Descent with RMSProp\n",
        "W, b, COST = rmsprop_gradient_descent(X, y, W, b, learning_rate, beta, epsilon, num_iterations, batch_size)\n",
        "\n",
        "# Print the final trained weights, bias, and cost\n",
        "print(\"Final trained weights (W):\\n\", W)\n",
        "print(\"Final trained bias (b):\\n\", b)\n",
        "print(\"Final cost:\", COST[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWG2TeG_pI_p",
        "outputId": "8180f3e0-ad43-4609-a782-5d37ef0b3987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Cost = 5.4722984310588405\n",
            "Iteration 11: Cost = 1.7634024955897027\n",
            "Iteration 21: Cost = 0.18128679305790574\n",
            "Iteration 31: Cost = 0.026364956476657927\n",
            "Iteration 41: Cost = 0.020332328929202358\n",
            "Iteration 51: Cost = 0.019946785913099768\n",
            "Iteration 61: Cost = 0.019911622016639274\n",
            "Iteration 71: Cost = 0.019861247956377955\n",
            "Iteration 81: Cost = 0.019946916833448486\n",
            "Iteration 91: Cost = 0.020201443505368625\n",
            "Final trained weights (W):\n",
            " [[2.97148484]]\n",
            "Final trained bias (b):\n",
            " 2.029889332157668\n",
            "Final cost: 0.020102589303330153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "Nng23vcj1o84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent_sgd(W, b, alpha, dW, db):\n",
        "    '''\n",
        "    Stochastic Gradient Descent Algorithm:\n",
        "    W : Weight term\n",
        "    b : bias term\n",
        "    alpha : learning rate\n",
        "    dW : partial derivative of W value\n",
        "    db : partial derivative of b value\n",
        "    '''\n",
        "    # Update weights and bias using the learning rate and partial derivatives\n",
        "    W = W - alpha * dW\n",
        "    b = b - alpha * db\n",
        "    return W, b\n",
        "\n",
        "def cost(W, b, X, y):\n",
        "    m = len(X)\n",
        "    return 1 / (2 * m) * np.sum((np.dot(X, W) + b - y) ** 2)\n",
        "\n",
        "# Define input data X and target data y\n",
        "X = np.array([i for i in range(100)]).reshape(-1, 1)\n",
        "y = 2 * X + np.random.randn(100, 1)  # Adding some noise for a more realistic scenario\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.array([0]).reshape(-1, 1)\n",
        "b = np.array([0]).reshape(-1, 1)\n",
        "\n",
        "iterations = 100\n",
        "alpha = 0.01\n",
        "\n",
        "COST = []\n",
        "\n",
        "# Perform Stochastic Gradient Descent iterations\n",
        "m = len(X)\n",
        "for i in range(iterations):\n",
        "    for j in range(m):\n",
        "        # Select a single data point\n",
        "        X_data_point = X[j]\n",
        "        y_data_point = y[j]\n",
        "\n",
        "        # Calculate the prediction for the single data point\n",
        "        prediction = np.dot(X_data_point, W) + b\n",
        "\n",
        "        # Calculate partial derivatives for the single data point\n",
        "        dW = X_data_point * (prediction - y_data_point)\n",
        "        db = prediction - y_data_point\n",
        "\n",
        "        # Update weights and bias using Stochastic Gradient Descent\n",
        "        W, b = gradient_descent_sgd(W, b, alpha, dW, db)\n",
        "\n",
        "        # Calculate the cost for the single data point and store it in the COST list\n",
        "        J = cost(W, b, X_data_point, y_data_point)\n",
        "        COST.append(J)\n",
        "\n",
        "    print(\"Iteration:\", i + 1)\n",
        "    print(\"Updated weights (W):\\n\", W)\n",
        "    print(\"Updated bias (b):\\n\", b)\n",
        "    print(\"Cost:\", J)\n",
        "\n",
        "# Print the final trained weights, bias, and cost\n",
        "print(\"Final trained weights (W):\\n\", W)\n",
        "print(\"Final trained bias (b):\\n\", b)\n",
        "print(\"Final cost:\", COST[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4mwcm8-1oeI",
        "outputId": "ecffadcb-02eb-4786-a620-a3ac5dc01f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1\n",
            "Updated weights (W):\n",
            " [[4.09683975e+116]]\n",
            "Updated bias (b):\n",
            " [[4.13779135e+114]]\n",
            "Cost: 8.226724527348943e+236\n",
            "Iteration: 2\n",
            "Updated weights (W):\n",
            " [[3.87270909e+228]]\n",
            "Updated bias (b):\n",
            " [[3.91142031e+226]]\n",
            "Cost: inf\n",
            "Iteration: 3\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 4\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 5\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 6\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 7\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 8\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 9\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 10\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 11\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 12\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 13\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 14\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 15\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 16\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 17\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 18\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 19\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 20\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 21\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 22\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 23\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 24\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 25\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 26\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 27\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 28\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-070d23fb4591>:19: RuntimeWarning: overflow encountered in square\n",
            "  return 1 / (2 * m) * np.sum((np.dot(X, W) + b - y) ** 2)\n",
            "<ipython-input-20-070d23fb4591>:46: RuntimeWarning: overflow encountered in multiply\n",
            "  dW = X_data_point * (prediction - y_data_point)\n",
            "<ipython-input-20-070d23fb4591>:13: RuntimeWarning: invalid value encountered in subtract\n",
            "  W = W - alpha * dW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "Iteration: 29\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 30\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 31\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 32\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 33\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 34\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 35\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 36\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 37\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 38\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 39\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 40\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 41\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 42\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 43\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 44\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 45\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 46\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 47\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 48\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 49\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 50\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 51\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 52\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 53\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 54\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 55\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 56\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 57\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 58\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 59\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 60\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 61\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 62\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 63\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 64\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 65\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 66\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 67\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 68\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 69\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 70\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 71\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 72\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 73\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 74\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 75\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 76\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 77\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 78\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 79\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 80\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 81\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 82\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 83\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 84\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 85\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 86\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 87\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 88\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 89\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 90\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 91\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 92\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 93\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 94\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 95\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 96\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 97\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 98\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 99\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Iteration: 100\n",
            "Updated weights (W):\n",
            " [[nan]]\n",
            "Updated bias (b):\n",
            " [[nan]]\n",
            "Cost: nan\n",
            "Final trained weights (W):\n",
            " [[nan]]\n",
            "Final trained bias (b):\n",
            " [[nan]]\n",
            "Final cost: [0.015422761210160008, 4.196354357626053, 10.798096305455836, 10.899357413332881, 6.448009266539327, 15.610346202101626, 9.416862445012338, 3.9147237211351307, 1.320263726485025, 0.05440391299375169, 3.819688837270664e-06, 0.19179924061336764, 0.173401903018862, 0.0016728116634427417, 2.406532793380482, 10.247870028797884, 30.9391462029897, 131.52632545318372, 698.4033268458215, 5027.403153164053, 50119.49310439828, 649238.8548026498, 10558867.736814851, 213311495.11324906, 5283762940.585828, 158602264868.4665, 5710482705017.378, 244392378665901.78, 1.2331477655992362e+16, 7.282238240015773e+17, 4.999679728360954e+19, 3.966492281955296e+21, 3.6160841484791966e+23, 3.7688707236183856e+25, 4.4695911509731625e+27, 6.0048400013426096e+29, 9.102045883080568e+31, 1.5506966493377705e+34, 2.958819765704798e+36, 6.301796004096368e+38, 1.4934897650281685e+41, 3.9268989728072466e+43, 1.1423437085021836e+46, 3.666906133472097e+48, 1.295614769916255e+51, 5.026876057478674e+53, 2.136934939552074e+56, 9.931790328559991e+58, 5.0364358767128214e+61, 2.7812238790283793e+64, 1.6694018568583523e+67, 1.087249063442901e+70, 7.670159902595493e+72, 5.8516992509415035e+75, 4.820429676727353e+78, 4.2812130831159476e+81, 4.093567727793122e+84, 4.20815657274935e+87, 4.644724208636724e+90, 5.497317262528942e+93, 6.968340203400766e+96, 9.448860868135486e+99, 1.368998728971268e+103, 2.1169914823742798e+106, 3.4903006573567758e+109, 6.128931835605486e+112, 1.1451177232718468e+116, 2.2742486040263044e+119, 4.796677057425095e+122, 1.0734026262567423e+126, 2.5463691210431142e+129, 6.398010717148506e+132, 1.7012681879166243e+136, 4.783597080245053e+139, 1.4211835851422857e+143, 4.45788557875605e+146, 1.4752625996579676e+150, 5.147054366651667e+153, 1.891877197059199e+157, 7.321089861823643e+160, 2.980703305278562e+164, 1.2759717093288856e+168, 5.739431324580523e+171, 2.7110363230356e+175, 1.343941082547111e+179, 6.98794853018932e+182, 3.80886906624426e+186, 2.1750913460626242e+190, 1.3006427228021261e+194, 8.139674679740209e+197, 5.328431364670468e+201, 3.6468249217415058e+205, 2.608185852560518e+209, 1.9483182721801863e+213, 1.5194041803710147e+217, 1.2364528240743867e+221, 1.049483738394325e+225, 9.286992752780663e+228, 8.564205704579962e+232, 8.226724527348943e+236, 8.39030153544844e+228, 8.221706350218395e+232, 2.9683389517632543e+233, 5.448022256104547e+233, 6.7355243047316635e+233, 5.830278535692071e+233, 3.3821085101331006e+233, 1.1755274113469723e+233, 1.9504040710072907e+232, 8.703177462562479e+230, 1.5201401125504755e+227, 1.3082522439306774e+228, 3.0379116516749414e+228, 3.867118170907442e+228, 4.6393832196565344e+228, 4.70899547616832e+228, 5.166770108083039e+228, 4.472958435835099e+228, 5.926895188356401e+228, 2.211995265555148e+228, 1.793711839455736e+229, 3.0783442506477656e+229, 1.071400787618005e+231, 1.8520762063579947e+232, 4.747803982563847e+233, 1.4158198635476582e+235, 5.103579445097056e+236, 2.1837809973570188e+238, 1.1019156398756064e+240, 6.507235121332531e+241, 4.467597140158266e+243, 3.544364754396589e+245, 3.231248259205611e+247, 3.367774768922238e+249, 3.9939221613992934e+251, 5.365784642667829e+253, 8.133375411400214e+255, 1.385666273305518e+258, 2.643932170665214e+260, 5.6311375844250054e+262, 1.3345475388821525e+265, 3.5089851181537356e+267, 1.020771631943756e+270, 3.2766615950962045e+272, 1.1577310692172655e+275, 4.4918989254989964e+277, 1.9095151042275217e+280, 8.874815649923993e+282, 4.5004413564749783e+285, 2.4852366382086254e+288, 1.491738471628287e+291, 9.71540345134057e+293, 6.853875574198071e+296, 5.228941648271173e+299, 4.3074232660105605e+302, 3.825591923888877e+305, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Regularization\n",
        "\n",
        "A)Dropout"
      ],
      "metadata": {
        "id": "44dulHplpOiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset (similar to previous examples)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create a Sequential model with dropout layers\n",
        "model_dropout = Sequential()\n",
        "model_dropout.add(Dense(128, input_shape=(28 * 28,), activation='relu'))\n",
        "model_dropout.add(Dropout(0.5))  # Dropout with a rate of 0.5\n",
        "model_dropout.add(Dense(64, activation='relu'))\n",
        "model_dropout.add(Dropout(0.5))  # Dropout with a rate of 0.5\n",
        "model_dropout.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model as before\n",
        "model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_dropout.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_dropout.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (Dropout Regularization): {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3WKcqPKpOSU",
        "outputId": "46cca76c-7966-4fe0-fcfc-eb4bee5ea54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 7s 6ms/step - loss: 0.7133 - accuracy: 0.7783 - val_loss: 0.2229 - val_accuracy: 0.9363\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.3657 - accuracy: 0.8976 - val_loss: 0.1649 - val_accuracy: 0.9516\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2972 - accuracy: 0.9173 - val_loss: 0.1457 - val_accuracy: 0.9574\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.2586 - accuracy: 0.9271 - val_loss: 0.1388 - val_accuracy: 0.9613\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.2366 - accuracy: 0.9329 - val_loss: 0.1249 - val_accuracy: 0.9658\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2211 - accuracy: 0.9377 - val_loss: 0.1185 - val_accuracy: 0.9675\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.2058 - accuracy: 0.9411 - val_loss: 0.1122 - val_accuracy: 0.9690\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 0.1965 - accuracy: 0.9441 - val_loss: 0.1096 - val_accuracy: 0.9703\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1917 - accuracy: 0.9465 - val_loss: 0.1058 - val_accuracy: 0.9712\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.1790 - accuracy: 0.9488 - val_loss: 0.1091 - val_accuracy: 0.9717\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9708\n",
            "Test Loss (Dropout Regularization): 0.1069, Test Accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B)L1(Lasso)"
      ],
      "metadata": {
        "id": "wjW8cawnpahS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l1\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset (similar to previous examples)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create a Sequential model with L1 regularization\n",
        "model_l1 = Sequential()\n",
        "model_l1.add(Dense(128, input_shape=(28 * 28,), activation='relu', kernel_regularizer=l1(0.01)))\n",
        "# The `kernel_regularizer` argument adds L1 regularization to the weights of this layer.\n",
        "# The parameter `0.01` specifies the strength of L1 regularization (adjust as needed).\n",
        "model_l1.add(Dense(64, activation='relu', kernel_regularizer=l1(0.01)))  # Add L1 regularization here too\n",
        "model_l1.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model as before\n",
        "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_l1.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_l1.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (L1 Regularization): {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eAzKm6QpaJh",
        "outputId": "974beadf-6cb1-45c5-ba73-5a7022921258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 8s 7ms/step - loss: 4.0388 - accuracy: 0.7714 - val_loss: 1.3959 - val_accuracy: 0.8385\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 1.3471 - accuracy: 0.8426 - val_loss: 1.2485 - val_accuracy: 0.8605\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 1.2353 - accuracy: 0.8556 - val_loss: 1.1601 - val_accuracy: 0.8707\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 1.1623 - accuracy: 0.8621 - val_loss: 1.0925 - val_accuracy: 0.8753\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 1.1160 - accuracy: 0.8659 - val_loss: 1.0658 - val_accuracy: 0.8764\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 1.0844 - accuracy: 0.8682 - val_loss: 1.0519 - val_accuracy: 0.8761\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 1.0581 - accuracy: 0.8723 - val_loss: 1.0092 - val_accuracy: 0.8842\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 1.0388 - accuracy: 0.8734 - val_loss: 1.0102 - val_accuracy: 0.8790\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 1.0193 - accuracy: 0.8749 - val_loss: 0.9951 - val_accuracy: 0.8832\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 1.0012 - accuracy: 0.8785 - val_loss: 0.9716 - val_accuracy: 0.8840\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9692 - accuracy: 0.8879\n",
            "Test Loss (L1 Regularization): 0.9692, Test Accuracy: 0.8879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) L2(Ridge)"
      ],
      "metadata": {
        "id": "ydNe02tWpjL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset (similar to previous examples)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create a Sequential model with L2 regularization\n",
        "model_l2 = Sequential()\n",
        "model_l2.add(Dense(128, input_shape=(28 * 28,), activation='relu', kernel_regularizer=l2(0.01)))\n",
        "# The `kernel_regularizer` argument adds L2 regularization to the weights of this layer.\n",
        "# The parameter `0.01` specifies the strength of L2 regularization (adjust as needed).\n",
        "model_l2.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # Add L2 regularization here too\n",
        "model_l2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model as before\n",
        "model_l2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_l2.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_l2.evaluate(X_test, y_test)\n",
        "print(f'Test Loss (L2 Regularization): {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns4MmvaRpi-J",
        "outputId": "730b008a-2858-4961-cdd3-6a418967d125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 5s 5ms/step - loss: 1.0123 - accuracy: 0.8820 - val_loss: 0.5885 - val_accuracy: 0.9159\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.5531 - accuracy: 0.9185 - val_loss: 0.5166 - val_accuracy: 0.9230\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.4890 - accuracy: 0.9296 - val_loss: 0.4420 - val_accuracy: 0.9427\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.4512 - accuracy: 0.9342 - val_loss: 0.4160 - val_accuracy: 0.9426\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 0.4198 - accuracy: 0.9394 - val_loss: 0.3995 - val_accuracy: 0.9433\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3965 - accuracy: 0.9438 - val_loss: 0.3771 - val_accuracy: 0.9488\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.9457 - val_loss: 0.3648 - val_accuracy: 0.9496\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3613 - accuracy: 0.9491 - val_loss: 0.3377 - val_accuracy: 0.9553\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.3481 - accuracy: 0.9505 - val_loss: 0.3247 - val_accuracy: 0.9557\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.3350 - accuracy: 0.9516 - val_loss: 0.3233 - val_accuracy: 0.9548\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3203 - accuracy: 0.9564\n",
            "Test Loss (L2 Regularization): 0.3203, Test Accuracy: 0.9564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) MNIST digit classification leveraging Convolutional Neural Networks (CNN)."
      ],
      "metadata": {
        "id": "r8oYbh6qprTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules from TensorFlow/Keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    MaxPool2D,\n",
        "    Flatten,\n",
        "    Dropout,\n",
        "    BatchNormalization\n",
        ")\n",
        "\n",
        "# Define the number of classes in the MNIST dataset\n",
        "num_classes = 10\n",
        "\n",
        "# Load the MNIST dataset and split it into training and validation sets\n",
        "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
        "\n",
        "# Convert the target labels (y_train and y_valid) to one-hot encoded format\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
        "\n",
        "# Normalize the pixel values of the images to a range between 0 and 1\n",
        "x_train = x_train / 255\n",
        "x_valid = x_valid / 255\n",
        "\n",
        "# Check the shapes of the training and validation data\n",
        "x_train.shape, x_valid.shape\n",
        "# Output: ((60000, 28, 28), (10000, 28, 28))\n",
        "\n",
        "# Reshape the image data to have a single channel (grayscale) dimension\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_valid = x_valid.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Check the shape of the reshaped training data\n",
        "x_train.shape\n",
        "# Output: (60000, 28, 28, 1)\n",
        "\n",
        "# Check the shape of the reshaped validation data\n",
        "x_valid.shape\n",
        "# Output: (10000, 28, 28, 1)\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first convolutional layer with 75 filters, a 3x3 kernel, \"same\" padding, ReLU activation, and input shape\n",
        "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "\n",
        "# Add batch normalization after the first convolutional layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max-pooling layer with a 2x2 pool size and \"same\" padding\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "\n",
        "# Add the second convolutional layer with 50 filters, a 3x3 kernel, \"same\" padding, and ReLU activation\n",
        "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# Add a dropout layer with a 20% dropout rate\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Add batch normalization after the second convolutional layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max-pooling layer with a 2x2 pool size and \"same\" padding\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "\n",
        "# Add the third convolutional layer with 25 filters, a 3x3 kernel, \"same\" padding, and ReLU activation\n",
        "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "# Add batch normalization after the third convolutional layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add max-pooling layer with a 2x2 pool size and \"same\" padding\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "\n",
        "# Flatten the output from the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 512 units and ReLU activation\n",
        "model.add(Dense(units=512, activation=\"relu\"))\n",
        "\n",
        "# Add a dropout layer with a 30% dropout rate\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add the output layer with the number of classes (10) and softmax activation\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss and accuracy as the evaluation metric\n",
        "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model on the training data for 5 epochs, using the validation data for validation\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_valid, y_valid))"
      ],
      "metadata": {
        "id": "Jefkj818prrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df34cc19-ec87-4eac-a440-ceec5bac96fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 75)        750       \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 28, 28, 75)        300       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 75)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 50)        33800     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 14, 14, 50)        0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 14, 14, 50)        200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 50)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 25)          11275     \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 7, 7, 25)          100       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 25)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               205312    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256867 (1003.39 KB)\n",
            "Trainable params: 256567 (1002.21 KB)\n",
            "Non-trainable params: 300 (1.17 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.1304 - accuracy: 0.9624 - val_loss: 0.0473 - val_accuracy: 0.9851\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0590 - accuracy: 0.9848 - val_loss: 0.0406 - val_accuracy: 0.9887\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0478 - accuracy: 0.9872 - val_loss: 0.0348 - val_accuracy: 0.9894\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0291 - val_accuracy: 0.9914\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0322 - accuracy: 0.9914 - val_loss: 0.0363 - val_accuracy: 0.9907\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bfd45eb1e10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Classification of 'Cats' and 'Dogs' images using CNN.\n"
      ],
      "metadata": {
        "id": "fmW1QVdepvBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the libraries\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "classifier= Sequential() # Initialise the CNN\n",
        "\n",
        "# Ist step of Convoltional layer to get feature maps using feature detector\n",
        "classifier.add(Convolution2D(filters=32, # output feature maps\n",
        "                             kernel_size=(3,3), # matrix size for feature detector\n",
        "                             input_shape=(64, 64, 3), # input image shape, 3 is for rgb coloured image with 128*128 px\n",
        "                             kernel_initializer='he_uniform', # weights distriution\n",
        "                             activation='relu')) # activation function\n",
        "\n",
        "# 2nd Pooling layer\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#2nd convolutional and pooling layer.\n",
        "classifier.add(Convolution2D(filters=32,\n",
        "                             kernel_size=(3,3),\n",
        "                             kernel_initializer='he_uniform',\n",
        "                             activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#Step 4 full connection in which input we have from flattening\n",
        "\n",
        "classifier.add(Dense(units=128,kernel_initializer='glorot_uniform', activation='relu'))\n",
        "#step 5 output layer\n",
        "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#applying all the transformation we want to apply to training data set\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "#Rescling the test data set images to use for validation.\n",
        "test_datagen= ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#Getting My training data ready for validation, so it will read all the data with the px size we gave.\n",
        "\n",
        "training_set= train_datagen.flow_from_directory(directory= '/content/drive/MyDrive/dataset',\n",
        "                                               target_size=(64,64), # As we choose 64*64 for our convolution model\n",
        "                                               batch_size=50,\n",
        "                                               class_mode='binary' # for 2 class binary\n",
        "                                               )\n",
        "\n",
        "#Getting My test data ready for validation, so it will read all the data with the px size we gave.\n",
        "\n",
        "test_set= test_datagen.flow_from_directory(directory= '/content/drive/MyDrive/dataset',\n",
        "                                               target_size=(64,64), # As we choose 64*64 for our convolution model\n",
        "                                               batch_size=50,\n",
        "                                               class_mode='binary' # for 2 class binary\n",
        "                                          )\n",
        "\n",
        "classifier.fit_generator(training_set, #training data to fit\n",
        "                        steps_per_epoch=100, # Data in training set (Changed from 1000 to 100)\n",
        "                        epochs=5, # No of epochs to run (changed from 10 to 5)\n",
        "                        validation_data=test_set, # Test or validation set\n",
        "                        validation_steps=1000 # no of data point for validation (I changed the validation steps from 2000 to 1000 to save time)\n",
        "                        )\n",
        "# Part 3 - Making new predictions\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/MyDrive/dataset/test_set/cats/cat.4002.jpg', target_size = (64, 64))\n",
        "# Loading the image and converting the pixels into array whcih will be used as input to predict.\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = classifier.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "    print(prediction)\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "    print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRwa-FSipyHF",
        "outputId": "bc4cce11-9451-4732-f152-220432abdec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10010 images belonging to 2 classes.\n",
            "Found 10010 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-eb8b1cd16b74>:66: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  classifier.fit_generator(training_set, #training data to fit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.7900"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 278s 3s/step - loss: 0.5395 - accuracy: 0.7900 - val_loss: 0.5109 - val_accuracy: 0.8002\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 21s 211ms/step - loss: 0.5205 - accuracy: 0.7922\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 24s 242ms/step - loss: 0.5074 - accuracy: 0.7992\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 21s 207ms/step - loss: 0.5033 - accuracy: 0.8014\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 22s 218ms/step - loss: 0.5024 - accuracy: 0.8030\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "dog\n"
          ]
        }
      ]
    }
  ]
}